import prepare

word2index = prepare.word2index

# 全単語数を取得
VOCAB_SIZE = len(word2index)
# 単語のベクトル数
EMBEDDING_DIM = 10
test = "ユージの前に立ちはだかったJOY「僕はAKBの高橋みなみを守る」"
# 単語IDの系列データに変換
inputs = prepare.sentence2index(test)
# 各単語のベクトルをまとめて取得
embeds = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)
sentence_matrix = embeds(inputs)
print(sentence_matrix.size())
print(sentence_matrix)
#torch.Size([13, 10])
#tensor([[ 0.5991,  0.2086,  1.6805, -0.2688, -0.5661,  1.0238, -0.8815,  2.0745, 0.8218, -1.0922],
#        [-0.7200,  1.3530, -1.7728, -0.3340, -0.2927, -0.2114,  0.1669,  1.4174, 1.0367, -0.1559],
#        [ 2.0492, -0.0129, -0.1688, -0.4127, -1.8662,  0.6761,  0.0921,  0.3018, 0.0510, -0.9186],
#        [-0.0932, -0.4891,  0.5047, -0.2488, -2.6789,  0.3175,  0.4011,  0.9005, 0.8657, -0.7729],
#        [ 0.6532,  0.8718, -0.6497,  0.5400, -0.1419,  0.8451, -0.5677,  0.1743, -0.0216,  0.8146],
#        [-1.2233, -0.9399,  0.2994,  0.9843,  0.6436, -0.1621,  0.6975, -0.4586, 0.9937, -0.4859],
#        [ 1.1178, -1.2890,  0.6551, -0.3249, -0.1036, -0.4176, -1.6938, -0.6290, -2.7653, -0.1765],
#        [ 0.5090,  1.4671, -0.8971,  1.3293, -0.5948, -1.7585,  0.0609,  0.1469, -0.9665, -0.4266],
#        [-0.7200,  1.3530, -1.7728, -0.3340, -0.2927, -0.2114,  0.1669,  1.4174, 1.0367, -0.1559],
#        [ 0.6907,  1.8703,  0.1093, -0.2989, -0.7074, -0.1824, -1.1053,  0.6469, -1.0702,  2.3492],
#        [ 1.1241, -0.8715,  0.4012, -0.5327, -0.1104,  1.7967, -0.9907,  1.4248, -1.7789,  1.6670],
#        [ 0.2470,  1.8372,  0.9765,  0.5153,  0.0936,  0.2957, -1.7517, -0.0556, -2.0370, -0.7433],
#        [-0.3896,  1.6902, -2.0145, -0.0156,  0.4149,  0.7111,  1.3389, -0.1780, -1.5560, -1.0672]], 
#        grad_fn=<EmbeddingBackward>)